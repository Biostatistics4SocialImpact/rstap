---
title: "STAP in a Longitudinal Data Setting"
author: ""
date: ""
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = T
)
```



Spatial-temporal aggregated predictor (STAP) models extension in the longitudinal setting incorporates heirarchical variance components to account for within subject or group-level correlation. 

This setting also involves a substantially different data structure, since exposure to built environment features now occurs across time and subjects can move across time or space. Additionally since subjects may drop out of a study and built-environment features - stores -  may close or open, the staps are neccessarily different at each measurement. The inclusion of these repeated measures, requires a secondary id to join on in addition to the typical ```id_key``` for the ```distance_data``` and ```time_data``` arguments to the ```stap_glmer``` function. We'll demonstrate this and the stap model with random effects using the simple case of homogenously distributed built environment features, with subjects measured across two time points.

```{r libraries,warning=F,message=F,results='hide'}
library(tidyverse)
library(ggthemes)
library(rstap)
library(plot3D)
library(gridExtra)
```

```{r datasetup, echo = F}
set.seed(25124)
num_subj_init <- 3.5E2
num_bef_init <- 50
possible_dates <- seq(from=as.Date('1970/01/01'), as.Date('1990/01/01'), by="day")
initial_dates <- sample(possible_dates,size = num_subj_init,replace = T)
pos_dates_1 <- seq(from=as.Date('2000/01/01'), as.Date('2000/12/31'), by="day")
pos_dates_2 <- seq(from=as.Date('2001/01/01'),as.Date('2003/12/31'),by='day')
time_1 <- (initial_dates - initial_dates)
subj_data <- tibble(x = runif(min = -1, max = 1, n = num_subj_init),
                        y = runif(min = -1, max = 1, n = num_subj_init),
                        date = initial_dates,
                        ID = 1:num_subj_init,
                        class = "Subject")

num_move <- rbinom(n = 1,size = num_subj_init, prob = 0.1)
movers <- sample(1:num_subj_init,size = num_move,replace = T) ## allow for individuals to move twice
pos_dates_moved <- seq(from=as.Date('1990/01/02'),as.Date('2003/12/31'),by='day')
dates_moved <- sample(pos_dates_moved,size = num_move,replace = T)
subj_data <- rbind(subj_data,
                   tibble(x = runif(min = -1,max = 1, n = num_move),
                              y = runif(min = -1,max = 1, n = num_move),
                              date = dates_moved,
                              ID = movers,
                              class = "Subject"))

## all businesses present before subjects in environment
possible_dates <- seq(from=as.Date('1960/01/01'), as.Date('1969/12/31'), by="day") 
bef_data <- tibble(x = runif(min = -1, max = 1, n = num_bef_init),
                       y = runif(min = -1, max = 1, n = num_bef_init),
                       date_open = sample(possible_dates,size = num_bef_init),
                       date_close = as.Date(NA),
                       ID = 1:num_bef_init,
                       class = "Coffee_Shop")

num_close <- rbinom(n = 1, size = num_bef_init, prob = 0.1)
close_id <- sample(1:num_bef_init,size = num_close,replace=F)
dates_closed <- sample(seq(from=as.Date('1970/01/01'),
                           as.Date('2003/12/31'), by="day"),
                       size = num_close)
num_open <- rbinom(n = 1,size = num_bef_init, prob = 0.1)
dates_open <- sample(seq(from=as.Date('1970/01/01'),
                           as.Date('2003/12/31'), by="day"),
                       size = num_open)
start <- num_bef_init+1
end <- (start+num_open) - 1
bef_data <- rbind(bef_data,
                  tibble(x = runif(min = -1, max = 1, n = num_open),
                             y = runif(min = -1, max = 1, n = num_open),
                             date_open = dates_open,
                             date_close = as.Date(NA),
                             ID = start:end,
                             class = "Coffee_Shop"))
bef_data[close_id,]$date_close = as.Date(dates_closed)

DOBS <- sample(seq(from=as.Date('1952/01/01'), as.Date('1969/12/31'), by="day"),
               size = num_subj_init,replace = T)
subj_fdata <- tibble(ID = 1:num_subj_init,
                         sex = rbinom(n = num_subj_init,size = 1,prob = .5),
                         DOB = DOBS,
                         Income = rlnorm(n = num_subj_init,meanlog = 4,sdlog = .5),
                         measure_date = sample(pos_dates_1,size = num_subj_init,replace =T))
num_drop <- rbinom(n = 1,size = num_subj_init,prob = .1)
subj_dropped <- sample(setdiff(1:num_subj_init,movers),size = num_drop,replace = F) ## don't drop movers to make it easier
subj_keep <- setdiff(1:num_subj_init,subj_dropped)
dates_2 <- sample(pos_dates_2,size = length(subj_keep),replace = T)
new_income <- subj_fdata[subj_keep,]$Income + 
    rnorm(n = (num_subj_init - num_drop),
          mean = (dates_2-initial_dates[subj_keep])*.01,sd = 2)
subj_fdata <- rbind(subj_fdata,tibble(ID = subj_keep,
                         sex = subj_fdata[subj_keep,]$sex,
                         DOB = subj_fdata[subj_keep,]$DOB,
                         Income = new_income,
                         measure_date = dates_2)) %>%
    mutate(Age= as.numeric((measure_date - DOB)/365),
           measure_ID = (measure_date>as.Date('2000/12/31'))*1+1)
```
## Measurement Data

Below we can see what the data, when initially measured, might look like before preparing it for model fitting.

```{r display_raw_distance}
knitr::kable(head(subj_data))
```

```{r display_bef_data}
knitr::kable(head(bef_data))
```


The third and final dataset involves the subject specific data and the dates at which the measurements were taken. 


```{r display_raw_subject}
subj_fdata %>% arrange(ID) %>% head() %>% knitr::kable()
```


## Transforming the Measured Data

In order to join these datasets, a new ID has to be created to associate the spatial-temporal data
with a specific measurement date. We create this ID to include all spatial data up to and including the measurement date for each subject.

Note that for subjects one and two there are two rows, corresponding to the two measurements.
In contrast, subject nine has four rows, corresponding to the fact that subject 9's move before the two measurements results in both locations of 9's residence being associated with all measurements subsequently measured. 

```{r demonstrate_measureIdmatch, measure_ID}
subj_data %>% left_join(subj_fdata,by="ID") %>% 
    filter(ID %in% c(1,2,9)) %>% 
    select(ID,measure_ID,x,y,date,measure_date) %>% knitr::kable()
```


```{r create_mID, echo = F}
subj_mdata <- subj_data %>% left_join(subj_fdata,by="ID") %>% 
     select(x,y,class,ID,measure_ID,date)

first_exposure <- subj_data %>% dplyr::filter(ID %in% movers) %>% group_by(ID) %>% 
    summarise(first_exposure=(max(date)-min(date))/365,
              move_date = min(date))

td_data <- bef_data %>% crossing(subj_fdata) %>% 
    left_join(subj_mdata,by = c("ID1"="ID","measure_ID"))  %>% 
    mutate(dist = sqrt((x.x - x.y)^2 + (y.x-y.y)^2),
           time_open = (measure_date - pmax(date_open,date)),
           time_close = date_close - pmax(date_open,date),
           time = pmin(time_open,time_close,na.rm = T)/365,
           time = as.numeric(time)*(time>0),
           subj_ID = ID1,
           bef_ID = ID,
           class = class.x) %>% 
    select(subj_ID,measure_ID,bef_ID,measure_date,
           date_open,date_close,date,class,dist,time) %>% 
    left_join(first_exposure,by=c("subj_ID"="ID")) %>% 
    mutate(move_date = replace_na(move_date,as.Date("1900-01-01")),
           first_exposure = replace_na(first_exposure,1), 
           time = (move_date==date)*first_exposure + (move_date!=date)*time) %>% 
    select(-first_exposure,-move_date)

```

## Stores Opening and Closing, Subjects Moving in the Data
Here we can see the effect of a store closing  (BEF_ID 10) and a subject moving
(subj_ID 9) on the data structure.
Note that first row's time is ~25.8 years which reflects the time spent since the *subject* first moved to their location. 
Since neither the subject nor the business moves by the subject's second measurement 
in July of 2001, we see that the distances betwen subject 1 and Coffee Shop 1 stay constant,
and the time increases according to the difference in measurement date, only.

In contrast, we see the effect of a subject moving for subj_ID 9. Here (s)he has twice as many measurements
 to show her/his exposure to the same coffee shop at the different locations where subj 9 lived. 
This is shown most evidently in the constant exposure for for coffee shops at the 1985 location for subject 9. Since subject 9 moved after ~6 years, and 
coffee shop 1 hadn't closed at that time, subject 9 only has 6 years of exposure to that coffee shop at that distance.
Following this we can see a steadily increasing exposure until the coffee shop closes in 2002, roughly a year before the subject's second measurement in '03.

```{r display_matched_data}
td_data  %>% 
    filter(subj_ID %in% c(1,9),bef_ID%in%c(1,10)) %>% 
    arrange(subj_ID,measure_ID,bef_ID) %>% 
    knitr::kable()
```


# Simulating and Estimating a STAP model


If we were to assume the following model from the previous data set-up

$$
E[Y_{ij}|b_i] = \alpha + Z_1\delta_{Income} + Z_{2}\delta_{sex} + Z_{3}\delta_{Age} + X(\theta_s,\theta_t)\beta_{Coffee} + b_i
$$
Where
$$
X(\theta_s,\theta_t) = \sum_{d\in\mathcal{D}} w_d(\frac{d}{\theta_s}) w_t(\frac{t}{\theta_t}) 
$$

and simulate it under the following fixed values

```{r true_pars}
alpha <- 22
delta <- c(Income = -.35,sex = 1.1, Age = 1.2)
beta <- 1
theta_s <- .8
theta_t <- 18
sigma <- 2
tau <- 1.5
d <- seq(from = 0, to = max(td_data$dist), by = 0.01)
t <- seq(from = 0, to = as.numeric(max(td_data$time)), by = 0.05)
w_s <- pracma::erfc(d/theta_s)
w_t <- pracma::erf(t/theta_t)
```


We could see the individual spatial and temporal exposure functions
```{r s-t-exp}
par(mfrow=c(1,2))
plot(d,w_s,type='l', main="Spatial Decay",xlab = "Distance", ylab ="Exposure")
plot(t,w_t,type = "l", main = "Temporal Accumulation",xlab='years', ylab = "Exposure")
```


As well as the joint spatial-temporal exposure surface:
```{r st_exp_plot,echo=F}
st_exp <- matrix(NA,nrow=length(d),ncol=length(t))
for(d_ix in 1:length(d)){
    for(t_ix in 1:length(t))
        st_exp[d_ix,t_ix] <- pracma::erfc(d[d_ix]/theta_s)*pracma::erf(t[t_ix]/theta_t)
}
persp3D(d,t,st_exp,phi = 15,theta = 45, xlab="Distance",
        ylab="Time (years)", zlab = "Exposure", ticktype='detailed',
        main = "Spatial Temporal Exposure Across Space and Time")
```




This results in the following exposure density across the population. Note that the exposure density value increases between the two measurements. This reflects the increased impact as a result of spending more time in the same space. With real data this may not be the case, given that individuals 

```{r total_exposure, echo = F}
td_data %>% group_by(subj_ID,measure_ID,date) %>% 
    summarize(total_exposure = sum(pracma::erfc(dist/theta_s) * pracma::erf(as.numeric(time)/theta_t)) ) %>% 
    ungroup() %>% 
    mutate(measure_ID = factor(measure_ID)) %>% 
    gather(contains("Exposure"),key="Exposure_Type",value = "Exposure") %>% 
    filter(Exposure_Type == "total_exposure") %>% 
    ggplot(aes(x=Exposure,fill= measure_ID )) + geom_density(alpha=0.3)  + 
        theme_bw() + ggtitle("Exposure Density Across Time")
```


```{r create_exposure,warning=F, echo = F,message=F}
Xs <- td_data %>% group_by(subj_ID,measure_ID,date) %>% 
    summarize(total_exposure = sum(pracma::erfc(dist/theta_s) *
                                       pracma::erf(as.numeric(time)/theta_t)) ) %>% 
    as_tibble() %>% gather(contains("Exposure"),key='Exposure_Type',value="Exposure") %>% 
    dplyr::filter(Exposure_Type == "total_exposure") 

Xs <- Xs %>% 
    group_by(subj_ID,measure_ID) %>% 
    summarise(Coffee_Shop = sum(Exposure))

subj_fdata <- subj_fdata %>% left_join(Xs,
                                       by=c("ID"="subj_ID","measure_ID"))

subj_fdata <- subj_fdata %>% 
    left_join(tibble(ID=1:num_subj_init,
                         ran_int = rnorm(n=num_subj_init,mean = 0, sd = tau)))
```


```{r, echo = F}
lme_inf <- lme4::glFormula(formula =  measure_ID ~
                               I((Income-mean(Income))/sd(Income) ) + 
                               sex + 
                               I((Age -mean(Age))/sd(Age)) + 
                               Coffee_Shop + (1|ID),
                     data = subj_fdata,family = gaussian(link='identity'))
Z <- lme_inf$X
y <- Z %*% c(alpha,delta,beta) + subj_fdata$ran_int + rnorm(n = nrow(Z),mean = 0,sd = sigma)
mu <- binomial(link='logit')$linkinv(Z %*% c(alpha-30,delta,beta) + subj_fdata$ran_int)
y_bern <- rbinom(n = length(y),size = 1,prob = mu) ## could also fit a bernoulli model
par(mfrow=c(1,2))
hist(y[1:num_subj_init], xlab = "BMI",main="First Measurement")
hist(y[num_subj_init+1:length(y)], xlab= "BMI", main="Second Measurement")
```


```{r}
subj_fdata <- subj_fdata %>% left_join(cbind(lme_inf$fr,y,y_bern),by=c("ID","measure_ID"))  %>% 
    mutate(sex = sex.x, Coffee_Shop= Coffee_Shop.x,subj_ID=ID,
           centered_income = `I((Income - mean(Income))/sd(Income))`,
           centered_age = `I((Age - mean(Age))/sd(Age))`) %>%
    select(subj_ID,DOB,Coffee_Shop,sex,
           centered_income,centered_age,y,y_bern,
           ran_int,measure_ID,Age,Income)
```


The final datasets passed to the ```stap_glmer``` function will then be the subject specific data, and then the distance and/or time dataset corresponding to whether or not a spatial or temporal scale is estimated.
```{r final_subj,echo=F}
select(subj_fdata,subj_ID,Income,measure_ID,Age,ran_int,y,sex,Coffee_Shop) %>% 
         head() %>%  knitr::kable()
```

```{r final_dist,echo=F}
select(td_data,subj_ID,measure_ID,date,bef_ID,class,dist) %>% 
         head() %>% knitr::kable()
```

```{r final_time,echo=F}
select(td_data,subj_ID,measure_ID,date,bef_ID,class,time) %>% 
    head() %>% knitr::kable()
```


```{r,echo=F}
## Names under which data is saved for package
homog_longitudinal_subject_data <- subj_fdata %>% 
    mutate(measure_ID = as.integer(measure_ID))
homog_longitudinal_bef_data <- td_data %>%
    mutate(measure_ID = as.integer(measure_ID))
homog_longitudinal_distance_data <- td_data %>% 
    select(subj_ID,measure_ID,bef_ID,class,dist) %>% 
    mutate(measure_ID = as.integer(measure_ID))
homog_longitudinal_time_data <- td_data %>% 
    select(subj_ID,measure_ID,bef_ID,class,time) %>% 
    mutate(measure_ID = as.integer(measure_ID),
           time = as.numeric(time))
```


We'll fit the data with priors that reflect our relative uncertainty about the spatial and temporal scales and place a fairly uninformative prior on the subject-specific variance. For information on how priors are set on the subject specific covariance, see the ```rstanarm``` vignette's write up [here](http://mc-stan.org/rstanarm/articles/glmer.html).
```{r fit_model}
fit_long <- stap_lmer(y ~ centered_income +  sex + centered_age + stap(Coffee_Shop) + (1|subj_ID),
                  subject_data = homog_longitudinal_subject_data, ## names of datasets
                  distance_data = homog_longitudinal_distance_data,##  simulated above
                  time_data = homog_longitudinal_time_data, ## in the rstap package
                  subject_ID = 'subj_ID',
                  group_ID = 'measure_ID',
                  prior_intercept = normal(location = 25, scale = 4, autoscale = F),
                  prior = normal(location = 0, scale = 4,autoscale=F),
                  prior_stap = normal(location = 0, scale = 4),
                  prior_theta = list(Coffee_Shop = list(spatial = log_normal(location = 1,
                                                                             scale = 1),
                                                        temporal = log_normal(location = 1,
                                                                              scale = 1))),
                  prior_covariance = decov(regularization = 1,concentration = 1,
                                          shape = 1,scale = 1),
                  prior_aux = cauchy(0,5),
                  max_distance = 3,
                  max_time = 50,
                  chains = 4,
                  iter = 2E3,
                  cores = 4)
```

we can now look at our estimates.  


```{r, echo = F}
as_tibble(fit_long) %>% select(-contains("b"),-contains("Sigma"),-contains("Intercept")) %>% 
  gather(everything(),key = "Parameter", value = "Samples") %>% 
  group_by(Parameter) %>% 
  summarise_all(list(lower = ~ quantile(.,0.05),
                     median = ~ median(.),
                     upper = ~quantile(.,0.95) 
                     ) ) %>% 
  mutate(Truth = (Parameter == "(Intercept)")*alpha +
                 (Parameter == "centered_income")*delta[1] + 
                 (Parameter == "sex")*delta[2] +
                 (Parameter == "centered_age")*delta[3] +
                 (Parameter == "Coffee_Shop") * beta+
                 (Parameter == "Coffee_Shop_spatial_scale") *theta_s + 
           (Parameter == "Coffee_Shop_temporal_scale") *theta_t,
         is_scale = factor(grepl(pattern = "scale",Parameter), labels = c("Effects", "Scales"))
         ) %>% 
  ggplot(aes(x=Parameter,y=median)) + geom_pointrange(aes(ymin=lower,ymax=upper)) + 
  theme_hc() + geom_point(aes(x=Parameter,y=Truth),color="red",size=2.5) + 
  coord_flip() + 
  labs(y = "Estimates", x = "Parameter", title = "Posterior Inference", subtitle = "90 % CI's") + 
  theme(strip.background = element_blank()) + geom_hline(aes(yintercept = 0),linetype=2)
```

We see that even though the temporal scale was quite a fair bit away from the bulk of the prior, the log normal tail and sufficiently data heavy likelihood allowed for a tight posterior interval around the temporal scale. Other parameter estimates were all accurate as well.

Visualizations of the estimated scales and random effects can be seen below.

```{r, echo = F, warning = F, message=F}
t <- seq(from = 0, to = as.numeric(max(td_data$time)),by=0.05)
d <- seq(from = 0, to = max(td_data$dist), by = 0.05)

as_tibble(fit_long) %>% 
  select(contains("scale")) %>% 
  mutate(prior = rlnorm(700,1,1)) %>% 
  gather(everything(), key = "Parameter", value = "Samples") %>% 
  group_by(Parameter) %>% 
  summarise_all(list(
                     med = ~ median(.)))
  spread(Parameter,med)
  
  

```




```{r,echo=F}
as_tibble(fit_long) %>% 
  select(contains("b[")) %>% 
  gather(everything(), key = "Parameter", value = "Samples") %>% 
  group_by(Parameter) %>% 
  summarise_all(list(lower = ~quantile(.,0.025),
                     med = ~median(.),
                     upper = ~quantile(.,0.975))) %>% 
  mutate(id = stringr::str_split(Parameter,"ID:"),
         id = map_int(id,function(x) as.integer(stringr::str_replace(x[2],"]","") ) ) ) %>% 
  arrange(id) %>% left_join(subj_fdata,by=c("id"="subj_ID")) %>% 
  ggplot(aes(x=ran_int,y=med)) + geom_point() + theme_hc() + 
  geom_abline(intercept=0,slope = 1) + 
  labs(title = "Random Effects", subtitle  = "Estimates vs. Truth", y = "Estimates", x = "Truth")
```



